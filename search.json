[
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "William Russell Lee",
    "section": "",
    "text": "Supply Chain (Operations) Management Student with Data Science Minor\n\nw.r.lee217@gmail.com | +1 (480) 353-0673 | LinkedIn\n\n\n\nSep 2022 - Jul 2025\nBrigham Young University - Idaho Rexburg, Idaho\n\nBachelors of Science in Supply Chain (Operations) Management\n\nData Science Minor\n\nOrganizations: Supply Chain Management Society & Data Science Society\n\n\n\n\nAug 2024 - Present\nMerchandising Assistant Intership Country Supplier LLC, Ammon, Idaho\nGenerated weekly savings of $50,000 by analyzing monthly sales data and inventory levels, informing purchasing decisions and optimizing stock management for pet consumables, food, and lawn and garden equipment. Collaborated with vendors and buyers daily to ensure accurate and timely deliveries. Resolved daily issues for CAL Ranch and Coastal stores related to receiving, special orders, and cancellations, streamlining operations.\nMay 2023 - Aug 2024\nSales Solution Specialist Safe Streets USA, Rexburg, Idaho\nConducted daily analysis of client accounts to maintain compliance and identify areas for improvement, averaging 10 accounts per day. Reviewed at least 5 equipment orders and delivery schedules daily to ensure timely arrival of technicians and fulfillment of client needs. Built strong relationships with technicians, sales reps, and clients, fostering collaboration and productivity.\nJan 2023 - Apr 2023\nFront of House Team Member Chick-Fil-A, Queen Creek, Arizona\nManaged efficient receipt and placement of over 1800 customer orders monthly. Expedited and delivered more than 900 customer orders promptly, proactively adding missing items as necessary. Conducted regular reviews of inventory for cups, lids, sauces, and utensils every 30 minutes to prevent stockouts and ensure uninterrupted service.\n\n\n\n\nProgramming Languages: SQL (Basic), Python (Basic), VBA (Basic)\nExcel: XLOOKUP, Macros, Pivot Tables\nLanguages: English (Native), Spanish (Intermediate)\n\n\n\n\nAug 2020 - Jul 2022\nFull Time Service Volunteer Church of Jesus Christ of Latter-day Saints, Salt Lake City, USA | Santiago, Chile\nEmployed proactive outreach strategies, including door knocking and referrals, to connect with community members and offer volunteer service and guidance. Analyzed progression data of community members and weekly volunteer activity to identify trends and optimize support strategies.\nJan 2017 - Jun 2018\nEagle Scout Project Boy Scouts of America, Mesa, Arizona\nLed a hygiene drive ensuring foster children had access to essential hygiene necessities. Coordinated distribution of informational fliers to over 500 households in a densely populated area. Delivered over 50 bags of hygiene items to Helenâ€™s Hope Chest, supporting underprivileged children."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "William Russell Lee",
    "section": "",
    "text": "Sep 2022 - Jul 2025\nBrigham Young University - Idaho Rexburg, Idaho\n\nBachelors of Science in Supply Chain (Operations) Management\n\nData Science Minor\n\nOrganizations: Supply Chain Management Society & Data Science Society"
  },
  {
    "objectID": "resume.html#work-experience",
    "href": "resume.html#work-experience",
    "title": "William Russell Lee",
    "section": "",
    "text": "Aug 2024 - Present\nMerchandising Assistant Intership Country Supplier LLC, Ammon, Idaho\nGenerated weekly savings of $50,000 by analyzing monthly sales data and inventory levels, informing purchasing decisions and optimizing stock management for pet consumables, food, and lawn and garden equipment. Collaborated with vendors and buyers daily to ensure accurate and timely deliveries. Resolved daily issues for CAL Ranch and Coastal stores related to receiving, special orders, and cancellations, streamlining operations.\nMay 2023 - Aug 2024\nSales Solution Specialist Safe Streets USA, Rexburg, Idaho\nConducted daily analysis of client accounts to maintain compliance and identify areas for improvement, averaging 10 accounts per day. Reviewed at least 5 equipment orders and delivery schedules daily to ensure timely arrival of technicians and fulfillment of client needs. Built strong relationships with technicians, sales reps, and clients, fostering collaboration and productivity.\nJan 2023 - Apr 2023\nFront of House Team Member Chick-Fil-A, Queen Creek, Arizona\nManaged efficient receipt and placement of over 1800 customer orders monthly. Expedited and delivered more than 900 customer orders promptly, proactively adding missing items as necessary. Conducted regular reviews of inventory for cups, lids, sauces, and utensils every 30 minutes to prevent stockouts and ensure uninterrupted service."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "William Russell Lee",
    "section": "",
    "text": "Programming Languages: SQL (Basic), Python (Basic), VBA (Basic)\nExcel: XLOOKUP, Macros, Pivot Tables\nLanguages: English (Native), Spanish (Intermediate)"
  },
  {
    "objectID": "resume.html#volunteer-experience",
    "href": "resume.html#volunteer-experience",
    "title": "William Russell Lee",
    "section": "",
    "text": "Aug 2020 - Jul 2022\nFull Time Service Volunteer Church of Jesus Christ of Latter-day Saints, Salt Lake City, USA | Santiago, Chile\nEmployed proactive outreach strategies, including door knocking and referrals, to connect with community members and offer volunteer service and guidance. Analyzed progression data of community members and weekly volunteer activity to identify trends and optimize support strategies.\nJan 2017 - Jun 2018\nEagle Scout Project Boy Scouts of America, Mesa, Arizona\nLed a hygiene drive ensuring foster children had access to essential hygiene necessities. Coordinated distribution of informational fliers to over 500 households in a densely populated area. Delivered over 50 bags of hygiene items to Helenâ€™s Hope Chest, supporting underprivileged children."
  },
  {
    "objectID": "notebooks/Exploration_04.html",
    "href": "notebooks/Exploration_04.html",
    "title": "Data Exploration 04",
    "section": "",
    "text": "Youâ€™re working with a team of botanists to develop a flower classification system.\nYour assignment is to build a k-Nearest Neighbors model to classify flowers based on their petal and sepal sizes."
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-a-import-and-explore-the-data",
    "href": "notebooks/Exploration_04.html#part-a-import-and-explore-the-data",
    "title": "Data Exploration 04",
    "section": "Part A: Import and Explore the data",
    "text": "Part A: Import and Explore the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/iris.csv\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\nimport pandas as pd\nfrom plotnine import *\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/iris.csv\")\n\n\ndf.columns\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n       'species'],\n      dtype='object')"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-b-visualize-the-data",
    "href": "notebooks/Exploration_04.html#part-b-visualize-the-data",
    "title": "Data Exploration 04",
    "section": "Part B: Visualize the Data",
    "text": "Part B: Visualize the Data\nUse your preferred visualization library to create a scatterplot showing petal length vs petal width. You should plot each flower species as a different color on the scatter plot.\n\n# Assuming your DataFrame is called df\nplot = (\n    ggplot(df, aes(x='petal_length', y='petal_width', color='species')) +\n    geom_point(size=2, alpha=0.7) +\n    labs(\n        title=\"Petal Length vs Petal Width by Species\",\n        x=\"Petal Length (cm)\",\n        y=\"Petal Width (cm)\",\n        color=\"Species\"\n    ) +\n    theme_minimal()\n)\n\ndisplay(plot)"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-c-prepare-the-data-for-machine-learning",
    "href": "notebooks/Exploration_04.html#part-c-prepare-the-data-for-machine-learning",
    "title": "Data Exploration 04",
    "section": "Part C: Prepare the Data for Machine Learning",
    "text": "Part C: Prepare the Data for Machine Learning\nData preparation (sometimes called â€œdata wranglingâ€ or â€œdata mungingâ€) is where youâ€™ll usually spend the bulk of your time when working on machine learning problems. Only rarely is data already in the optimal form for a given algorithm.\nOften we have to deal with missing values, normalize the data, and perform both simple and complex feature engineering to get the data into the form we need.\nOnce the data is in the correct form, we can then randomize the data and split it into training and test datasets (and sometimes an additional validation dataset).\n\nMachine Learning Steps\nAlmost universally, regardless of which algorithm or type of task weâ€™re performing, building and evaluating a machine learning model with sklearn follows these steps:\n\nPerform any data preprocessing needed.\nPartition the data into features and targets.\nSplit the data into training and test sets (and sometimes a third validation set).\nCreate a configure whichever sklearn model object weâ€™re using.\nTrain the model using its â€œfitâ€ method.\nTest the model using its â€œpredictâ€ method.\nUse a model evaluation metric to see how well the model performs.\n\nIf the model isnâ€™t performing well, we will repeat one or more of the above steps (sometimes all of them).\nOnce the model is performing adequately, weâ€™ll deploy it for use as part of some larger system.\nFor now, letâ€™s assume that this dataset is in the form we need, and weâ€™ll skip to step 2, partitioning the data.\n\n\nStep 2. Partition the Data into Features and Targets\nFirst, weâ€™ll create a dataframe called â€œXâ€ containing the features of the data we want to use to make our predictions. In this case, that will be the sepal_length, sepal_width, petal_length, and petal_width features.\n(The name â€œXâ€ isnâ€™t special, but uppercase X is the conventional name for our feature dataset, because thatâ€™s what statisticians use to refer to a matrix of independent variables)\n\n# Create a new dataframe called X that contians the features we're going\n# to use to make predictions\nX = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n\nNext weâ€™ll create a dataframe called â€œyâ€ containing the target variable, or the set of values we want to predict. In this case, that will be species.\n(Once again, the name â€œyâ€ isnâ€™t special, but lowercase y is the conventional name for a list of targets, because thatâ€™s what statisticians use to refer to a vector of dependent variables)\n\n# Create a new dataframe called y that contians the target we're\n# trying to predict\n\ny = df['species']\n\n\n\n\nStep 3. Split the data into training and test sets.\nNow that we have our data divided into features (X) and target values (y), weâ€™ll split each of these into a training set and a test set.\nWeâ€™ll use the training sets to â€œtrainâ€ our model how to make predictions.\nWeâ€™ll then use our test sets to test how well our model has learned from the training data.\nWhile we could use a bunch of python code to do this step, the sklearn library has lots of built-in functions to handle common data manipulations related to machine learning.\nFor this step, weâ€™ll use the train_test_split() function.\n\n# Import and use the train_test_split() function to split the X and y\n# dataframes into training and test sets.\n#\n# The training data should contain 80% of the samples and\n# the test data should contain 20% of the samples.\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nAfter creating the training and test splits, output the head() of each one and notice how they row numbers have been randomized.\nAlso notice that X_train and y_trainâ€™s row numbers match up, as do X_test and y_testâ€™s row numbers.\n\n# Inspect the first few rows of each split\nprint(\"X_train:\\n\", X_train.head(), \"\\n\")\nprint(\"y_train:\\n\", y_train.head(), \"\\n\")\nprint(\"X_test:\\n\", X_test.head(), \"\\n\")\nprint(\"y_test:\\n\", y_test.head())\n\nX_train:\n     sepal_length  sepal_width  petal_length  petal_width\n22           4.6          3.6           1.0          0.2\n15           5.7          4.4           1.5          0.4\n65           6.7          3.1           4.4          1.4\n11           4.8          3.4           1.6          0.2\n42           4.4          3.2           1.3          0.2 \n\ny_train:\n 22        Iris-setosa\n15        Iris-setosa\n65    Iris-versicolor\n11        Iris-setosa\n42        Iris-setosa\nName: species, dtype: object \n\nX_test:\n      sepal_length  sepal_width  petal_length  petal_width\n73            6.1          2.8           4.7          1.2\n18            5.7          3.8           1.7          0.3\n118           7.7          2.6           6.9          2.3\n78            6.0          2.9           4.5          1.5\n76            6.8          2.8           4.8          1.4 \n\ny_test:\n 73     Iris-versicolor\n18         Iris-setosa\n118     Iris-virginica\n78     Iris-versicolor\n76     Iris-versicolor\nName: species, dtype: object"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-d-create-and-train-a-model",
    "href": "notebooks/Exploration_04.html#part-d-create-and-train-a-model",
    "title": "Data Exploration 04",
    "section": "Part D: Create and Train a Model",
    "text": "Part D: Create and Train a Model\nWeâ€™re going to create a model based on the k-Nearest Neighbors algorithm.\nSince this is a classification task, (weâ€™re trying to classify which species a given flower belongs to), weâ€™ll use sklearnâ€™s KNeighborsClassifer.\n\nStep 4. Create and configure the model\nWe start by importing the information about the model we want to create. In python, this information is called a class.\nThe KNeighborsClassifier class contains all of the information python needs to create a kNN Classifier.\nOnce weâ€™ve imported the class, weâ€™ll create an instance of the class using this syntax:\nwhatever = ClassName( parameter_one = value, parameter_two = something_else, etc...)\nIn our case, the class name is KNeighborsClassifer. It doesnâ€™t matter what we call the variable that holds the instance, but one popular convention is to call classifier instances clf, so thatâ€™s what youâ€™ll see in the sklearn documentation.\nThe only parameter we want to configure is the n_neighbors parameter, which controls the value of k in the kNN algorithm.\n\n# Import the KNeighborsClassifier class from sklearn\n# Note that it's in the neighbors submodule. See the example code in the\n# documentation for details on how to import it\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a kNN classifier instance with k=5\nclf = KNeighborsClassifier(n_neighbors=5)\n\n\n# Create an instance of the model, configuring it to use the 3 nearest neighbors\n# store the instance in a variable\n# Create an instance of the model using 3 nearest neighbors\nclf = KNeighborsClassifier(n_neighbors=3)\n\n\n\n\nStep 5: Train the model\nNext weâ€™ll train the model. We do this by providing it with the training data we split off from the dataset in step 3.\nThe model â€œlearnsâ€ how to associate the feature values (X) with the targets (y). The exact process it uses to learn how to do this depends on which algorithm weâ€™re using.\nSometimes, this is called â€œfitting the data to the modelâ€, so in sklearn, we perform this step using the fit() method.\n\n# Call the \"fit\" method of the classifier instance we created in step 4.\n# Pass it the X_train and y_train data so that it can learn to make predictions\n# Train the model on the training data\nclf.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3)"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-e-make-predictions-and-evaluate-the-model",
    "href": "notebooks/Exploration_04.html#part-e-make-predictions-and-evaluate-the-model",
    "title": "Data Exploration 04",
    "section": "Part E: Make Predictions and Evaluate the Model",
    "text": "Part E: Make Predictions and Evaluate the Model\nNow that the model has been created and trained, we can use it to make predictions. Since this is a classification model, when we give it a set of features, it tells us what the most likely target value is.\nIn this case, we tell the model â€œhere are the values for petal width, petal length, sepal width, and sepal length for a particular flowerâ€ The model then tells us which species is the most likely for that flower.\nWhen testing how well our model works, weâ€™ll use the test data we split off earlier. It contains the measurements for several flowers, along with their species.\n\nStep 6: Make Predictions on Test Data\nWeâ€™ll give the measurements of each flower to the model and have it predict their species. Weâ€™ll then compare those predictions to the known values to determine how accurate our model is.\nSince this is a classification model, there are two different methods we can use to make predictions:\n\npredict(), which returns the most likely prediction for each sample.\npredict_proba() which returns a list of probabilities for each sample. The probabilities tell us how confident the model is that the corresponding sample belongs to a particular class.\n\n\n# Use the predict() method to get a list of predictions for the samples in our\n# test data. Then output those predictions\n# Predict the species for the test data\ny_pred = clf.predict(X_test)\n\n\n\n# Just a quick comparison with y_test to see if they match up\n# Compare predicted vs actual species labels\ncomparison = pd.DataFrame({\n    'Actual': y_test.values,\n    'Predicted': y_pred\n})\n\n# Show the first few rows\nprint(comparison.head())\n\n\n            Actual        Predicted\n0  Iris-versicolor  Iris-versicolor\n1      Iris-setosa      Iris-setosa\n2   Iris-virginica   Iris-virginica\n3  Iris-versicolor  Iris-versicolor\n4  Iris-versicolor  Iris-versicolor\n\n\n\n\nStep 7: Evaluate the Model\nThere are several metrics we can use to determine how well our model is performing.\nMost of them are in the sklearn.metrics library.\nMost of the sklearn metric function work using the same pattern. We import the function, then give it a list of the true values for our test data, and a list of the values the model predicted for our test data. The metric then outputs the value. How we interpret that value will depend on the exact problem weâ€™re solving, the qualities of our data, and the particular metric weâ€™re using.\n\nAccuracy\nSince this is a multiclass classification problem (â€œmulticlassâ€ means we have more than two options weâ€™re choosing from), we can get a quick estimate from the accuracy_score() function, which tells us the percent of correct predictions made by the model.\n\n# Import the accuracy_score function and use it to determine\n# how accurate the models predictions were for our test data\nfrom sklearn.metrics import accuracy_score\n\n# Evaluate how accurate the model's predictions were\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the result as a percentage\nprint(f\"Model Accuracy: {accuracy:.2%}\")\n\nModel Accuracy: 100.00%\n\n\n\n\nConfusion Matrix\nWhile the accuracy score tells us a little about the modelâ€™s performance, it doesnâ€™t tell us much.\nFor example, we know how often the model was correct, but we donâ€™t know when it was wrong or why.\nWe can get this information from the confusion_matrix function.\n\n# Import the confusion_matrix function and use it to generate a confusion\n# matrix of our model results.\n\nfrom sklearn.metrics import confusion_matrix\n\n# Generate a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Display the confusion matrix\nprint(\"Confusion Matrix:\\n\", cm)\n\n\nConfusion Matrix:\n [[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\n\n\n\nConfusion Matrix Plot\nItâ€™s easier to see the results of the confusion matrix if we plot the results. One way to do this is with Seabornâ€™s heatmap function.\nThis function works a little bit differently than the others. It takes as parameters your model instance, and then options for making the chart display the way you want, and outputs a confusion matrix showing how well the model did in predicting the target values.\nYouâ€™ll notice that in many cases (including this one), the numbers in the confusion matrix will be the same as the results you see from the confusion_matrix() function above, but the plot makes it easier to interpret the results.\nWhen using the confusion matrix, you may find that the default color mapping is difficult to read. The â€œBluesâ€ mapping is a popular choice.\n\n# Create a Seaborn heatmap\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Create a new model instance\nclf_scaled = KNeighborsClassifier(n_neighbors=3)\n\n# Train on the scaled data\nclf_scaled.fit(X_train_scaled, y_train)\n\n# Predict\ny_pred_scaled = clf_scaled.predict(X_test_scaled)\n\n# Evaluate accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred_scaled)\nprint(f\"Accuracy after scaling: {accuracy:.2%}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-21-df3de3e3051a&gt; in &lt;cell line: 0&gt;()\n      9 \n     10 # Train on the scaled data\n---&gt; 11 clf_scaled.fit(X_train_scaled, y_train)\n     12 \n     13 # Predict\n\nNameError: name 'X_train_scaled' is not defined"
  },
  {
    "objectID": "notebooks/Exploration_04.html#above-and-beyond",
    "href": "notebooks/Exploration_04.html#above-and-beyond",
    "title": "Data Exploration 04",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nOnce youâ€™ve complted the basics, try to complete one or more of the following tasks:\n\nSee if you can get better results from your model through some data preprocessing, such as normalization.\nOften, using too many features can give poor results. Can you get better performance using a subset of the features instead of all four?\nAre there other ways you could visualize your model results?"
  },
  {
    "objectID": "notebooks/starter_bikes.html",
    "href": "notebooks/starter_bikes.html",
    "title": "William Lee - Data Science Portfolio",
    "section": "",
    "text": "# Core Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Scikit-learn for preprocessing and metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# TensorFlow / Keras for neural networks\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n# Load data\nbikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\n\nprint(daily['dteday'].max())\nprint(daily.tail())\n\n2023-10-31 00:00:00\n         dteday     temp_c  feels_like_c       hum  windspeed  holiday  \\\n4682 2023-10-27  20.225000     20.229167  0.730217   8.341667        0   \n4683 2023-10-28  21.537500     21.537500  0.712254   7.250000        0   \n4684 2023-10-29  20.595833     20.595833  0.780079   8.904167        0   \n4685 2023-10-30  17.129167     17.129167  0.762071  16.133333        0   \n4686 2023-10-31  10.258333      9.562500  0.513467  12.762500        0   \n\n      workingday  season  weathersit  casual  registered  count  day_of_week  \\\n4682           1       4    1.166667    6894       11689  18583            4   \n4683           0       4    1.041667    9505       10669  20174            5   \n4684           0       4    1.375000    7965        9199  17164            6   \n4685           1       4    1.625000    4005        9180  13185            0   \n4686           1       4    1.458333    4072       10630  14702            1   \n\n      month   day_sin   day_cos  month_sin  month_cos  \n4682     10 -0.433884 -0.900969  -0.866025        0.5  \n4683     10 -0.974928 -0.222521  -0.866025        0.5  \n4684     10 -0.781831  0.623490  -0.866025        0.5  \n4685     10  0.000000  1.000000  -0.866025        0.5  \n4686     10  0.781831  0.623490  -0.866025        0.5  \n\n\n\n#Chunk 1: Data Preparation (Updated for Dictionary)\n\n# Parse date and create datetime features\nbikes['dteday'] = pd.to_datetime(bikes['dteday'])\n\n# Group by day to get total rentals per day\ndaily = bikes.groupby('dteday').agg({\n    'temp_c': 'mean',\n    'feels_like_c': 'mean',\n    'hum': 'mean',\n    'windspeed': 'mean',\n    'holiday': 'first',\n    'workingday': 'first',\n    'season': 'first',\n    'weathersit': 'mean',\n    'casual': 'sum',\n    'registered': 'sum'\n}).reset_index()\n\n# Create total count\ndaily['count'] = daily['casual'] + daily['registered']\n\n# Time-based features\ndaily['day_of_week'] = daily['dteday'].dt.dayofweek\ndaily['month'] = daily['dteday'].dt.month\n\n# Cyclical encoding for day of week and month\ndaily['day_sin'] = np.sin(2 * np.pi * daily['day_of_week'] / 7)\ndaily['day_cos'] = np.cos(2 * np.pi * daily['day_of_week'] / 7)\ndaily['month_sin'] = np.sin(2 * np.pi * daily['month'] / 12)\ndaily['month_cos'] = np.cos(2 * np.pi * daily['month'] / 12)\n\n# Final features\nfeatures = ['temp_c', 'feels_like_c', 'hum', 'windspeed', 'holiday',\n            'workingday', 'season', 'weathersit', 'day_sin', 'day_cos',\n            'month_sin', 'month_cos']\ntarget = 'count'\n\n# Use October for testing\ntrain_bikes = daily[daily['dteday'] &lt; '2023-10-01']\ntest_bikes = daily[(daily['dteday'] &gt;= '2023-10-01') & (daily['dteday'] &lt;= '2023-10-31')]\n\n\nX_train = train_bikes[features]\ny_train = train_bikes[target]\nX_test = test_bikes[features]\ny_test = test_bikes[target]\n\n# Normalize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n\n#Chunk 2: Model Training, Prediction, and Visualization\n\n# Build the model\nmodel = Sequential([\n    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nhistory = model.fit(X_train_scaled, y_train,\n                    validation_split=0.2,\n                    epochs=100,\n                    callbacks=[early_stop],\n                    verbose=1)\n\n# Predict on test data (Novâ€“Dec)\npredictions = model.predict(X_test_scaled).flatten()\n\n# Create results DataFrame\nresults = test_bikes[['dteday']].copy()\nresults['predicted_count'] = predictions\nresults['actual_count'] = y_test.values\n\n# Save predictions to CSV\nresults.to_csv('bike_predictions_nov_dec.csv', index=False)\n\n# Plot predictions vs actuals\nplt.figure(figsize=(10,5))\nplt.plot(results['dteday'], results['actual_count'], label='Actual')\nplt.plot(results['dteday'], results['predicted_count'], label='Predicted')\nplt.xlabel('Date')\nplt.ylabel('Bike Rentals')\nplt.title('Predicted vs Actual Rentals (Novâ€“Dec)')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nEpoch 1/100\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2s 4ms/step - loss: 70424816.0000 - val_loss: 113714456.0000\n\nEpoch 2/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 69319712.0000 - val_loss: 108094120.0000\n\nEpoch 3/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 65332012.0000 - val_loss: 93672584.0000\n\nEpoch 4/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 53595780.0000 - val_loss: 71537792.0000\n\nEpoch 5/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 39475808.0000 - val_loss: 48906092.0000\n\nEpoch 6/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 25133076.0000 - val_loss: 33694528.0000\n\nEpoch 7/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 17806370.0000 - val_loss: 25685034.0000\n\nEpoch 8/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 14929171.0000 - val_loss: 21343044.0000\n\nEpoch 9/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 13379225.0000 - val_loss: 18579612.0000\n\nEpoch 10/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 11583489.0000 - val_loss: 16696055.0000\n\nEpoch 11/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 10681658.0000 - val_loss: 15350994.0000\n\nEpoch 12/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 10192391.0000 - val_loss: 14327760.0000\n\nEpoch 13/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 9464767.0000 - val_loss: 13581077.0000\n\nEpoch 14/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 9312943.0000 - val_loss: 12862096.0000\n\nEpoch 15/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 9067448.0000 - val_loss: 12379151.0000\n\nEpoch 16/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 8569479.0000 - val_loss: 12206759.0000\n\nEpoch 17/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 8200425.5000 - val_loss: 11845378.0000\n\nEpoch 18/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 8044232.0000 - val_loss: 11544975.0000\n\nEpoch 19/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 8199983.5000 - val_loss: 11423943.0000\n\nEpoch 20/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7992178.0000 - val_loss: 11225510.0000\n\nEpoch 21/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7937254.0000 - val_loss: 11010642.0000\n\nEpoch 22/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7774034.5000 - val_loss: 10963210.0000\n\nEpoch 23/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7418599.5000 - val_loss: 11082862.0000\n\nEpoch 24/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7302108.0000 - val_loss: 11027010.0000\n\nEpoch 25/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7468162.5000 - val_loss: 10925253.0000\n\nEpoch 26/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7667901.0000 - val_loss: 10691827.0000\n\nEpoch 27/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7577240.5000 - val_loss: 10710919.0000\n\nEpoch 28/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 7587403.0000 - val_loss: 10595903.0000\n\nEpoch 29/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7521169.5000 - val_loss: 10452062.0000\n\nEpoch 30/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7478130.0000 - val_loss: 10501903.0000\n\nEpoch 31/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7500408.0000 - val_loss: 10533403.0000\n\nEpoch 32/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7334494.0000 - val_loss: 10549941.0000\n\nEpoch 33/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7265127.5000 - val_loss: 10694477.0000\n\nEpoch 34/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7366901.5000 - val_loss: 10454693.0000\n\nEpoch 35/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7210457.0000 - val_loss: 10596307.0000\n\nEpoch 36/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 7338425.5000 - val_loss: 10472235.0000\n\nEpoch 37/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 6967924.0000 - val_loss: 10314968.0000\n\nEpoch 38/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 7318820.5000 - val_loss: 10270708.0000\n\nEpoch 39/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7068828.5000 - val_loss: 10651185.0000\n\nEpoch 40/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7181523.0000 - val_loss: 10474749.0000\n\nEpoch 41/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7175226.0000 - val_loss: 10387621.0000\n\nEpoch 42/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7219342.5000 - val_loss: 10123820.0000\n\nEpoch 43/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7197482.5000 - val_loss: 10085655.0000\n\nEpoch 44/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7163026.0000 - val_loss: 9921490.0000\n\nEpoch 45/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7312141.0000 - val_loss: 10088922.0000\n\nEpoch 46/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6910817.5000 - val_loss: 10365882.0000\n\nEpoch 47/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7308799.5000 - val_loss: 9977747.0000\n\nEpoch 48/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7313624.0000 - val_loss: 10212233.0000\n\nEpoch 49/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6759995.5000 - val_loss: 10160584.0000\n\nEpoch 50/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7037776.0000 - val_loss: 10263839.0000\n\nEpoch 51/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7218991.5000 - val_loss: 10135973.0000\n\nEpoch 52/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7408150.0000 - val_loss: 10003939.0000\n\nEpoch 53/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7018993.5000 - val_loss: 9859009.0000\n\nEpoch 54/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7004226.0000 - val_loss: 9934395.0000\n\nEpoch 55/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7328979.0000 - val_loss: 10153591.0000\n\nEpoch 56/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 6883759.5000 - val_loss: 9943611.0000\n\nEpoch 57/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7001976.5000 - val_loss: 10043222.0000\n\nEpoch 58/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 4ms/step - loss: 6947211.0000 - val_loss: 9815245.0000\n\nEpoch 59/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 7157318.0000 - val_loss: 9762445.0000\n\nEpoch 60/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 6977111.5000 - val_loss: 9830232.0000\n\nEpoch 61/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 7131560.0000 - val_loss: 10223201.0000\n\nEpoch 62/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 6967989.0000 - val_loss: 9954341.0000\n\nEpoch 63/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7139183.0000 - val_loss: 9955302.0000\n\nEpoch 64/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6830925.5000 - val_loss: 9841929.0000\n\nEpoch 65/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7112849.0000 - val_loss: 10012460.0000\n\nEpoch 66/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7124915.0000 - val_loss: 9493503.0000\n\nEpoch 67/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7223541.0000 - val_loss: 9616753.0000\n\nEpoch 68/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6878581.5000 - val_loss: 10126824.0000\n\nEpoch 69/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6650340.5000 - val_loss: 10029972.0000\n\nEpoch 70/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 7034530.5000 - val_loss: 9778288.0000\n\nEpoch 71/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6990830.0000 - val_loss: 9854739.0000\n\nEpoch 72/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6787386.5000 - val_loss: 9544423.0000\n\nEpoch 73/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 7070328.5000 - val_loss: 9662549.0000\n\nEpoch 74/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6939409.0000 - val_loss: 9968430.0000\n\nEpoch 75/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6815552.5000 - val_loss: 10148884.0000\n\nEpoch 76/100\n\n117/117 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 6747764.5000 - val_loss: 9848731.0000\n\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 70ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/module01-assessment.html",
    "href": "notebooks/module01-assessment.html",
    "title": "Introduction",
    "section": "",
    "text": "This assignment will test how well youâ€™re able to perform various data science-related tasks.\nEach Problem Group below will center around a particular dataset that you have worked with before.\nTo ensure you receive full credit for a question, make sure you demonstrate the appropriate pandas, altair, or other commands as requested in the provided code blocks.\nYou may find that some questions require multiple steps to fully answer. Others require some mental arithmetic in addition to pandas commands. Use your best judgment."
  },
  {
    "objectID": "notebooks/module01-assessment.html#submission",
    "href": "notebooks/module01-assessment.html#submission",
    "title": "Introduction",
    "section": "Submission",
    "text": "Submission\nEach problem group asks a series of questions. This assignment consists of two submissions:\n\nAfter completing the questions below, open the Module 01 Assessment Quiz in Canvas and enter your answers to these questions there.\nAfter completing and submitting the quiz, save this Colab notebook as a GitHub Gist (Youâ€™ll need to create a GitHub account for this), by selecting Save a copy as a GitHub Gist from the File menu above.\nIn Canvas, open the Module 01 Assessment GitHub Gist assignment and paste the GitHub Gist URL for this notebook. Then submit that assignment."
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-1",
    "href": "notebooks/module01-assessment.html#problem-group-1",
    "title": "Introduction",
    "section": "Problem Group 1",
    "text": "Problem Group 1\nFor the questions in this group, youâ€™ll work with the Netflix Movies Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\n\nQuestion 1\nLoad the dataset into a Pandas data frame and determine what data type is used to store the release_year feature.\n\nimport pandas as pd\n\nmovies = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\")\n\nprint(movies.dtypes[\"release_year\"])\n\nint64\n\n\n\n\nQuestion 2\nFilter your dataset so it contains only TV Shows. How many of those TV Shows were rated TV-Y7?\n\n\nonly_tv = movies[movies[\"type\"] == \"TV Show\"]\n\nonly_tv['rating'].value_counts()\n\n\n\n\n    pandas.core.frame.DataFrame.infodef info(verbose: bool | None=None, buf: WriteBuffer[str] | None=None, max_cols: int | None=None, memory_usage: bool | str | None=None, show_counts: bool | None=None) -&gt; None/usr/local/lib/python3.11/dist-packages/pandas/core/frame.pyPrint a concise summary of a DataFrame.\n\nThis method prints information about a DataFrame including\nthe index dtype and columns, non-null values and memory usage.\n\nParameters\n----------\nverbose : bool, optional\n    Whether to print the full summary. By default, the setting in\n    ``pandas.options.display.max_info_columns`` is followed.\nbuf : writable buffer, defaults to sys.stdout\n    Where to send the output. By default, the output is printed to\n    sys.stdout. Pass a writable buffer if you need to further process\n    the output.\nmax_cols : int, optional\n    When to switch from the verbose to the truncated output. If the\n    DataFrame has more than `max_cols` columns, the truncated output\n    is used. By default, the setting in\n    ``pandas.options.display.max_info_columns`` is used.\nmemory_usage : bool, str, optional\n    Specifies whether total memory usage of the DataFrame\n    elements (including the index) should be displayed. By default,\n    this follows the ``pandas.options.display.memory_usage`` setting.\n\n    True always show memory usage. False never shows memory usage.\n    A value of 'deep' is equivalent to \"True with deep introspection\".\n    Memory usage is shown in human-readable units (base-2\n    representation). Without deep introspection a memory estimation is\n    made based in column dtype and number of rows assuming values\n    consume the same memory amount for corresponding dtypes. With deep\n    memory introspection, a real memory usage calculation is performed\n    at the cost of computational resources. See the\n    :ref:`Frequently Asked Questions &lt;df-memory-usage&gt;` for more\n    details.\nshow_counts : bool, optional\n    Whether to show the non-null counts. By default, this is shown\n    only if the DataFrame is smaller than\n    ``pandas.options.display.max_info_rows`` and\n    ``pandas.options.display.max_info_columns``. A value of True always\n    shows the counts, and False never shows the counts.\n\nReturns\n-------\nNone\n    This method prints a summary of a DataFrame and returns None.\n\nSee Also\n--------\nDataFrame.describe: Generate descriptive statistics of DataFrame\n    columns.\nDataFrame.memory_usage: Memory usage of DataFrame columns.\n\nExamples\n--------\n&gt;&gt;&gt; int_values = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n&gt;&gt;&gt; float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n&gt;&gt;&gt; df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n...                   \"float_col\": float_values})\n&gt;&gt;&gt; df\n    int_col text_col  float_col\n0        1    alpha       0.00\n1        2     beta       0.25\n2        3    gamma       0.50\n3        4    delta       0.75\n4        5  epsilon       1.00\n\nPrints information of all columns:\n\n&gt;&gt;&gt; df.info(verbose=True)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   int_col    5 non-null      int64\n 1   text_col   5 non-null      object\n 2   float_col  5 non-null      float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes\n\nPrints a summary of columns count and its dtypes but not per column\ninformation:\n\n&gt;&gt;&gt; df.info(verbose=False)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nColumns: 3 entries, int_col to float_col\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes\n\nPipe output of DataFrame.info to buffer instead of sys.stdout, get\nbuffer content and writes to a text file:\n\n&gt;&gt;&gt; import io\n&gt;&gt;&gt; buffer = io.StringIO()\n&gt;&gt;&gt; df.info(buf=buffer)\n&gt;&gt;&gt; s = buffer.getvalue()\n&gt;&gt;&gt; with open(\"df_info.txt\", \"w\",\n...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n...     f.write(s)\n260\n\nThe `memory_usage` parameter allows deep introspection mode, specially\nuseful for big DataFrames and fine-tune memory optimization:\n\n&gt;&gt;&gt; random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n... })\n&gt;&gt;&gt; df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 22.9+ MB\n\n&gt;&gt;&gt; df.info(memory_usage='deep')\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 165.9 MB\n      \n      \n\n\n\n\nQuestion 3\nFurther filter your dataset so it only contains TV Shows released between the years 2000 and 2009 inclusive. How many of those shows were rated TV-Y7?\n\ntv_2000s = only_tv[(only_tv['release_year'] &gt;= 2000) & (only_tv['release_year'] &lt;= 2009)]\ntv_y7_count = tv_2000s[tv_2000s['rating'] == 'TV-Y7'].shape[0]\nprint(f\"Number of TV-Y7 shows released from 2000 to 2009: {tv_y7_count}\")\n\nNumber of TV-Y7 shows released from 2000 to 2009: 4"
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-2",
    "href": "notebooks/module01-assessment.html#problem-group-2",
    "title": "Introduction",
    "section": "Problem Group 2",
    "text": "Problem Group 2\nFor the questions in this group, youâ€™ll work with the Cereal Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\n\nQuestion 4\nAfter importing the dataset into a pandas data frame, determine the median amount of protein in cereal brands manufactured by Kelloggs. (mfr code â€œKâ€)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\n\ndf.columns\n\n# Filter for cereals made by Kellogg's\nkelloggs_cereals = df[df['mfr'] == 'K']\n\n# Calculate the median protein amount\nmedian_protein = kelloggs_cereals['protein'].median()\n\nprint(f\"Median protein for Kellogg's cereals: {median_protein}\")\n\nMedian protein for Kellogg's cereals: 3.0\n\n\n\n\nQuestion 5\nIn order to comply with new government regulations, all cereals must now come with a â€œHealthinessâ€ rating. This rating is calculated based on this formula:\nhealthiness = (protein + fiber) / sugar\nCreate a new healthiness column populated with values based on the above formula.\nThen, determine the median healthiness value for only General Mills cereals (mfr = â€œGâ€), rounded to two decimal places.\n\ndf['healthiness'] = (df['protein'] + df['fiber']) / df['sugars']\n\ngeneral_mills = df[df['mfr'] == 'G']\n\nmedian_healthiness = general_mills['healthiness'].median()\n\nmedian_healthiness_rounded = round(median_healthiness, 2)\n\nprint(f\"Median healthiness for General Mills cereals: {median_healthiness_rounded}\")\n\nMedian healthiness for General Mills cereals: 0.47"
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-3",
    "href": "notebooks/module01-assessment.html#problem-group-3",
    "title": "Introduction",
    "section": "Problem Group 3",
    "text": "Problem Group 3\nFor the questions in this group, youâ€™ll work with the Titanic Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\n\nQuestion 6\nAfter loading the dataset into a pandas DataFrame, create a new column called NameGroup that contains the first letter of the passengerâ€™s surname in lower case.\nNote that in the dataset, passengerâ€™s names are provided in the Name column and are listed as:\nSurname, Given names\nFor example, if a passengerâ€™s Name is Braund, Mr. Owen Harris, the NameGroup column should contain the value b.\nThen count how many passengers have a NameGroup value of k.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\")\n\ndf['NameGroup'] = df['Name'].str.split(',').str[0].str.lower().str[0]\n\nk_count = df[df['NameGroup'] == 'k'].shape[0]\n\n\nprint(f\"The amount of passengers have a NameGroup of : {k_count}\")\n\nThe amount of passengers have a NameGroup of : 28"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "notebooks/Exploration_02.html",
    "href": "notebooks/Exploration_02.html",
    "title": "Data Exploration 02",
    "section": "",
    "text": "Youâ€™re working as a data analyst at a cereal marketing company in New York.\nIn a strategy meeting, the marketing director tells you that in 2018, the US weight loss industry was worth over $72 Billion dollars, growing 4% compared to the previous year.\nIn contrast, sales of cold cereal fell 6% to $8.5 billion during the same time period.\nCereal executives have approached the marketing company asking how they can somehow tap into the weight loss market growth to boost the sales of their cereal brands.\nYour assignment is to analyze a dataset of nutritional information for major US cereals, and calculate some metrics that can be used by the marketing team."
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 02",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If youâ€™ve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\n\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\n\ndf.head()\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "href": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "title": "Data Exploration 02",
    "section": "Part 2: Calculate Summary Statistics",
    "text": "Part 2: Calculate Summary Statistics\nThe marketing team has determined that when choosing a cereal, consumers are most interested in calories, sugars, fiber, fat, and protein.\nFirst, letâ€™s calcuate some summary statistics for these categories across the entire dataset. Weâ€™re particularly intrested in the mean, median, standard deviation, min, and max values.\nThere are multiple ways to accomplish this.\n\n# Part 2: Enter your code below to calculate summary statistics for the\n# calories, sugars, fiber, fat, and protein features.\ndf_filtered = df[[\"calories\", \"sugars\",\"fiber\", \"fat\", \"protein\"]]\n\ndf_filtered.agg({\n        \"calories\": [\"min\", \"max\", \"median\", \"mean\", \"std\"],\n        \"sugars\": [\"min\", \"max\", \"median\", \"mean\", \"std\"],\n        \"fiber\": [\"min\", \"max\", \"median\", \"mean\", \"std\"],\n        \"fat\": [\"min\", \"max\", \"median\", \"mean\", \"std\"],\n        \"protein\": [\"min\", \"max\", \"median\", \"mean\", \"std\"]\n\n}).T\n\n\n\n\n    \n\n\n\n\n\n\nmin\nmax\nmedian\nmean\nstd\n\n\n\n\ncalories\n50.0\n160.0\n110.0\n106.883117\n19.484119\n\n\nsugars\n-1.0\n15.0\n7.0\n6.922078\n4.444885\n\n\nfiber\n0.0\n14.0\n2.0\n2.151948\n2.383364\n\n\nfat\n0.0\n5.0\n1.0\n1.012987\n1.006473\n\n\nprotein\n1.0\n6.0\n3.0\n2.545455\n1.094790"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-3-transform-data",
    "href": "notebooks/Exploration_02.html#part-3-transform-data",
    "title": "Data Exploration 02",
    "section": "Part 3: Transform Data",
    "text": "Part 3: Transform Data\nTo make analysis easier, you want to convert the manufacturer codes used in the dataset to the manufacturer names.\nFirst, display the count of each manufacturer code value used in the dataset (found in the mfr column).\nThen, create a new column with the appropriate manufacturer name for each entry, using this mapping:\nA = American Home Food Products\nG = General Mills\nK = Kelloggs\nN = Nabisco\nP = Post\nQ = Quaker Oats\nR = Ralston Purina\n\nNote: While the tutorial linked above uses the replace function, using the map function instead can often be much faster and more memory efficient, especially for large datasets.\n\n\n# Display the count of values for the manufacturer code (\"mfr\" column), then\n# create a new column containing the appropriate manufacturer names.\ndf[\"mfr\"] = df[\"mfr\"].map({\n    \"A\": \"American Home Food Products\",\n    \"G\": \"General Mills\",\n    \"K\": \"Kelloggs\",\n    \"N\": \"Nabisco\",\n    \"P\": \"Post\",\n    \"Q\": \"Quaker Oats\",\n    \"R\": \"Ralston Purina\"\n})"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-4-visualization",
    "href": "notebooks/Exploration_02.html#part-4-visualization",
    "title": "Data Exploration 02",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nLetâ€™s do some more data exploration visually.\nImport your visualization library of choice and set any needed configuration options.\n\n# Import your visualization library\nfrom plotnine import *\n\n\nSugar Distribution\nMarketing tells us that their surveys have revealed that sugar content is the number one concern of consumers when choosing cereal.\nThey would like to see the following visualizations:\n\nA histogram plot of the sugar content in all cereals.\nA scatter plot showing the relationship between sugar and calories.\nA box plot showing the distribution of sugar content by manufacturer.\n\n\n# Create the three visualzations requested by the the marketing team\nhisto_df = ggplot(df, aes(x = 'mfr')) + \\\n    geom_histogram()\ndisplay(histo_df)\n\nscatter_plot = ggplot(df, aes(x='calories', y='sugars')) + \\\n    geom_point(color='blue', alpha=0.6)\ndisplay(scatter_plot)\n\nbox_plot = ggplot(df, aes(x='mfr', y='sugars')) + \\\n    geom_boxplot(fill='orange', color='black')\ndisplay(box_plot)\n\n/usr/local/lib/python3.11/dist-packages/plotnine/stats/stat_bin.py:109: PlotnineWarning: 'stat_bin()' using 'bins = 5'. Pick better value with 'binwidth'."
  },
  {
    "objectID": "notebooks/Exploration_02.html#above-and-beyond",
    "href": "notebooks/Exploration_02.html#above-and-beyond",
    "title": "Data Exploration 02",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nThe marketing team is pleased with what youâ€™ve accomplished so far. They have a meeting with top cereal executives in the morning, and theyâ€™d like you to do as many of the following additional tasks as you have time for:\n\nWeight Watchers used to have an older points system that used this formula: (calories / 50) + (fat / 12) - (fiber / 5), but only the first 4 grams of fiber were included in the calculation. For comparisonâ€™s sake, create an additional column with the calculation for the old points system.\nMarketing really likes the boxplot of the sugar content for each cereal, theyâ€™d like similar plots for calories and fat, but using different color schemes for each chart.\n\n\ndf['fiber_capped'] = df['fiber'].clip(upper=4)\n\n# Calculate old points using the capped fiber\ndf['ww_old_points'] = (\n    (df['calories'] / 50) +\n    (df['fat'] / 12) -\n    (df['fiber_capped'] / 5)\n).round().astype(int)"
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html",
    "href": "notebooks/Exploration_01_Solved.html",
    "title": "Data Exploration 01",
    "section": "",
    "text": "A consumer watchdog group wants to see if Netflix has more movies for adults or children.\nUsing a dataset containing metadata for all of the movies Netflix had available on their platform in 2019, weâ€™ll use the MPAA movie rating system to determine if they are correct."
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#mpaa-movie-ratings",
    "href": "notebooks/Exploration_01_Solved.html#mpaa-movie-ratings",
    "title": "Data Exploration 01",
    "section": "MPAA Movie Ratings:",
    "text": "MPAA Movie Ratings:\n\nG: All ages admitted.\nPG: Some material may not be suitable for children.\nPG-13: Some material may be inappropriate for children under 13.\nR: Under 17 requires accompanying parent or adult guardian\nNC-17: No One 17 and Under Admitted\n\nMost people would consider G and PG as ratings suitable for children. However, not everyone would agree that a PG-13 movie is necssarily a childrenâ€™s movie. It is up to you to decide how to handle this."
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#part-1-import-pandas",
    "href": "notebooks/Exploration_01_Solved.html#part-1-import-pandas",
    "title": "Data Exploration 01",
    "section": "Part 1: Import Pandas",
    "text": "Part 1: Import Pandas\nThe pandas library is a python library used for data analysis and manipulation. It will provide the core functionality for most of what you do in the data exploration and preprocessing stages of most machine learning projects.\nPlease see this Getting Started Guide for information on the conventional way to import Pandas into your project, as well as other helpful tips for common Pandas tasks.\n\n# Part 1: Enter the code below to import Pandas according to the\n# conventional method.\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#part-2-load-the-data",
    "href": "notebooks/Exploration_01_Solved.html#part-2-load-the-data",
    "title": "Data Exploration 01",
    "section": "Part 2: Load the data",
    "text": "Part 2: Load the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 2: Load the dataset into a Pandas dataframe.\nnetflix = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv')\n\n\n# Then, explore the data by seeing what the first few rows look like.\n\n# Note the missing values for director in the first few rows. Missing values\n# here are represented by \"NaN\" (Not a Number)\nnetflix.head()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\n81145628\nMovie\nNorm of the North: King Sized Adventure\nRichard Finn, Tim Maltby\nAlan Marriott, Andrew Toth, Brian Dobson, Cole...\nUnited States, India, South Korea, China\nSeptember 9, 2019\n2019\nTV-PG\n90 min\nChildren & Family Movies, Comedies\nBefore planning an awesome wedding for his gra...\n\n\n1\n80117401\nMovie\nJandino: Whatever it Takes\nNaN\nJandino Asporaat\nUnited Kingdom\nSeptember 9, 2016\n2016\nTV-MA\n94 min\nStand-Up Comedy\nJandino Asporaat riffs on the challenges of ra...\n\n\n2\n70234439\nTV Show\nTransformers Prime\nNaN\nPeter Cullen, Sumalee Montano, Frank Welker, J...\nUnited States\nSeptember 8, 2018\n2013\nTV-Y7-FV\n1 Season\nKids' TV\nWith the help of three human allies, the Autob...\n\n\n3\n80058654\nTV Show\nTransformers: Robots in Disguise\nNaN\nWill Friedle, Darren Criss, Constance Zimmer, ...\nUnited States\nSeptember 8, 2018\n2016\nTV-Y7\n1 Season\nKids' TV\nWhen a prison ship crash unleashes hundreds of...\n\n\n4\n80125979\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\n\n\n\n\n\n\n\n\n# Next, display a technical summary of the data to determine the data types of each column, and which columns have missing data.\n\n# From the results of info(), it appears that most columns have some missing\n# values, with director, cast, country having the most missing\nnetflix.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6234 entries, 0 to 6233\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   show_id       6234 non-null   int64 \n 1   type          6234 non-null   object\n 2   title         6234 non-null   object\n 3   director      4265 non-null   object\n 4   cast          5664 non-null   object\n 5   country       5758 non-null   object\n 6   date_added    6223 non-null   object\n 7   release_year  6234 non-null   int64 \n 8   rating        6224 non-null   object\n 9   duration      6234 non-null   object\n 10  listed_in     6234 non-null   object\n 11  description   6234 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 584.6+ KB"
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#part-3-filter-the-data",
    "href": "notebooks/Exploration_01_Solved.html#part-3-filter-the-data",
    "title": "Data Exploration 01",
    "section": "Part 3: Filter the Data",
    "text": "Part 3: Filter the Data\nSince weâ€™re just interested in movies, weâ€™ll need to filter out anything that isnâ€™t a movie for our analysis. The type feature contains this information.\nOnce we have the subset, we should see how many rows it contains. There are a variety of ways to get the length of a data frame.\n\n# Use pandas's filtering abilitites to select the subset of data\n# that represents movies, then calculate how many rows are in the filtered data.\nmovies = netflix[ netflix['type'] == 'Movie']\nlen(movies)\n\n4265\n\n\n\nMPAA Ratings\nNow that we have only movies, letâ€™s get a quick count of the values being used in the rating feature.\n\n# Determine the number of records for each value of the \"rating\" feature.\n# Remember to count the values in your subset only, not in the original dataframe.\nmovies['rating'].value_counts()\n\nTV-MA       1348\nTV-14       1038\nR            506\nTV-PG        432\nPG-13        286\nNR           202\nPG           183\nTV-G          80\nTV-Y7         69\nTV-Y          41\nG             36\nTV-Y7-FV      27\nUR             7\nNC-17          2\nName: rating, dtype: int64\n\n\n\n\nMore Filtering\nThere are apparently some â€œmade for TVâ€ movies in the list that donâ€™t fit the MPAA rating scheme.\nLetâ€™s filter some more to just see movies rated with the standard MPAA ratings of G, PG, PG-13, R, and NC-17.\n\n# Filter the list of movies to select a new subset containing only movies with\n# a standard MPAA rating. Calculate how many rows are in this new set, and\n# then see which ratings appear most often.\n\n# Here is the list of ratings I want\nmpaa_ratings = ['G', 'PG', 'PG-13', 'R', 'NC-17']\n\n# Create a new dataframe containing only movies whose ratings are in that list\nmpaa_movies = movies[ movies['rating'].isin(mpaa_ratings) ]\n\n# See the new counts\nmpaa_movies['rating'].value_counts()\n\nR        506\nPG-13    286\nPG       183\nG         36\nNC-17      2\nName: rating, dtype: int64"
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#part-4-visualization",
    "href": "notebooks/Exploration_01_Solved.html#part-4-visualization",
    "title": "Data Exploration 01",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nNow that we have explored and preprocessed our data, letâ€™s create a visualization to summarize our findings.\n\nExploration vs Presentation\nBroadly speaking, there are two types of visualizations: * Barebones visualizations you might use to get a quick, visual understanding of the data while youâ€™re trying to decide how it all fits together. * Presentation-quality visualizations that you would include in a report or presentation for management or other stakeholders.\n\n\nVisualization Tools\nThere are many different visualization tools availble. In the sections below, weâ€™ll explore the three most common. Each of these libraries has strengths and weaknesses.\nIt is probably a good idea for you to become familiar with each one, and then become proficient at whichever one you like the best.\n\n\nAltair\nThe Altair visualization library provides a large variety of very easy to use statistical charting tools.\nAltair uses a declarative language to build up charts piece by piece.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nalt.Chart(employees).mark_box().encode(\n    x='job',\n    y='salary'\n)\n\n# Make a box plot style categorical plot, and customize the results\nalt.Chart(employees).mark_box().encode(\n    alt.X('job', title='Job title')\n    alt.Y('salary', title='Annual salary in thousands of $USD')\n).properties(\n  title='Salaries by Job Title'\n)\nLike with Pandas, there is a conventional way to import Altair into your projects.\n\n# Import the Altair library the conventional way.\nimport altair as alt\n\nLetâ€™s create a barchart showing the count of each movie rating by using Altairâ€™s aggregation capabilities.\nIn this example, we see the x axis being set to a feature called a, and the y axis set to the average() of a feature called b.\nIn our case, we want the x axis to be set to rating and the y axis to be the count() of rating.\n\n# Use Altair to create a bar chart comparing the count of each movie rating\n\n# Note that this is a \"barebones visualization\".\n# It's fine for a quick understanding of the data, but it's not something we'd\n# want to put in a report or presentation.\nalt.Chart(mpaa_movies).mark_bar().encode(\n    x='rating',\n    y='count(rating)'\n)\n\n\n\n\n\n\n\n# This is a \"presentation quality\" visualization.\n# - Each axis is labeled using proper title casing\n# - There's a properly formatted chart title\n# - Note that we turn off the legend because the information is redundant\n# - It's scaled up to a good size\n# - We're rearranging the category order on the x axis to map to how most people\n#   would think about this data.\nalt.Chart(mpaa_movies).mark_bar().encode(\n    alt.X('rating', title='MPAA Rating', sort=['G', 'PG', 'PG-13', 'R', 'NC-17']),\n    alt.Y('count(rating)', title='Number of Movies on Netflix'),\n    alt.Color('rating', legend=None)\n).properties(\n    title='Netflix Movie Ratings',\n    width=600\n)\n\n\n\n\n\n\n\n\nSeaborn\nWhile Altair uses a â€œdeclarativeâ€ syntax for building charts piece by piece, the Seaborn library provides a large variety of pre-made charts for common statistical needs.\nThese charts are divided into different categories. Each category has a high-level interface you can use for simplicity, and then a specific function for each chart that you can use if you need more control over how the chart looks.\nSeaborn uses matplotlib for its drawing, and the chart-specific functions each return a matplitlib axes object if you need additional customization.\nFor example, there are several different types of categorical plots in seaborn: bar plots, box plots, point plots, count plots, swarm plots, etcâ€¦\nEach of these plots can be accessed using the catplot function.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nsns.catplot(data=employees, x='job', y='salary', kind='box')\n\n# Make a swarm plot style categorical plot\nsns.catplot(data=employees, x='job', y='salary', kind='swarm')\nAlternatively, you can use the plot specific functions to give yourself more control over the output by using matplotlib functions:\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = sns.boxplot(data=employees, x='job', y='salary')\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\nLike with Pandas, there is a conventional way to import Seaborn into your projects.\nOptionally, you may wish to set some default chart aesthetics by setting the chart style.\n\n# Import the seaborn library the conventional way. Then optionally configure\n# the default chart style.\nimport seaborn as sns\n\n# Set some default styling\nsns.set()\n\n# Choose a theme\nsns.set_style('ticks')\n\nSince the rating column uses categorical data, we need to use Seabornâ€™s categorical visualizations.\nIn particular, we want a â€œcount plotâ€ that will display a count of movie ratings.\n\n# Use seaborn to create a count plot comparing the count of each movie rating\n\n# Note that this is a \"barebones visualization\".\n# It's fine for a quick understanding of the data, but it's not something we'd\n# want to put in a report or presentation.\nsns.catplot(data=mpaa_movies, x='rating', kind=\"count\")\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# This is a \"presentation quality\" visualization.\n# - Each axis is labeled using proper title casing\n# - There's a properly formatted chart title\n# - No legend is needed because the information would be redundant\n# - It's scaled up to a good size\n# - We're rearranging the category order on the x axis to map to how most people\n#   would think about this data.\nplt.figure(figsize=(10,6))\n\n# Switch from the generic catplot() to the more specific countplot() so we can customize things\nax = sns.countplot(data=mpaa_movies, x='rating', order=['G', 'PG', 'PG-13', 'R', 'NC-17'])\nax.set_title(\"Netflix Movie Ratings\")\nax.set_ylabel(\"Number of Movies on Netflix\")\nax.set_xlabel(\"MPAA Rating\")\n\nText(0.5, 0, 'MPAA Rating')\n\n\n\n\n\n\n\n\n\n\n\nPandas built-in plotting\nIn addition to libraries like Altair and Seaborn, Pandas has some built in charting functionality.\nWhile not as sophisticated as some of the other options, it is often good enough for quick visualizations.\nJust like with seabornâ€™s plotting functions, the pandas plotting functions return matplotlib axes objects, which can be further customized.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nemployees[ ['job','salary'] ].plot.box()\n\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = employees[ ['job','salary'] ].plot().box()\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\n\n# Use pandas' built in plotting functions to create a count plot comparing the count of each movie rating\n# This will be a little trickier than the other libraries, but one hint is that the pandas value_counts() function\n# actually returns a dataframe.\n\n# Note that this is a \"barebones visualization\".\n# It's fine for a quick understanding of the data, but it's not something we'd\n# want to put in a report or presentation.\nmpaa_movies['rating'].value_counts().plot.bar()\n\n\n\n\n\n\n\n\n\n\n# The latest version of pandas uses monochrome for a lot of charting.\n# So we'll create a list of colors to use\ncolors = ['#5875A4', '#CC8962', '#5F9E6E', '#B55D60', '#F58519']\n\n# Get the value counts as a dataframe and change the ordering\nrating_counts = mpaa_movies['rating'].value_counts().reindex(['G', 'PG', 'PG-13', 'R', 'NC-17'])\n\n# In pandas, we could actually supply figsize, title, xlabel, and ylabel as arguments to the plot function\n# But we separate them out here for better readability\nplt.figure(figsize=(10,6))\nax = rating_counts.plot.bar(color=colors)\nax.set_title(\"Netflix Movie Ratings\")\nax.set_ylabel(\"Number of Movies on Netflix\")\nax.set_xlabel(\"MPAA Rating\")\n\n# This is a \"presentation quality\" visualization.\n# - Each axis is labeled using proper title casing\n# - There's a properly formatted chart title\n# - No legend is needed because the information would be redundant\n# - It's scaled up to a good size\n# - We're rearranging the category order on the x axis to map to how most people\n#   would think about this data.\n\nText(0.5, 0, 'MPAA Rating')"
  },
  {
    "objectID": "notebooks/Exploration_01_Solved.html#above-and-beyond",
    "href": "notebooks/Exploration_01_Solved.html#above-and-beyond",
    "title": "Data Exploration 01",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nAfter reviewing your findings, the watchdog group would like some additional questions answered:\n\nHow are things affected if you include the â€œmade for TV moviesâ€ that have been assigned TV ratings in your analysis, but still exclude unrated movies?\nThey would also like to see a separate report that includes only TV shows.\nFor an upcoming community meeting, the group would like to present a simple chart showing â€œFor Kidsâ€ and â€œFor Adultsâ€ categories. The easiest way to accomplish this would be to create a new column in your data frame that maps each rating to the appropriate â€œFor Kidsâ€ or â€œFor Adultsâ€ label, then create a new visualization based on that column."
  },
  {
    "objectID": "notebooks/Exploration_03.html",
    "href": "notebooks/Exploration_03.html",
    "title": "Data Exploration 03",
    "section": "",
    "text": "Youâ€™re working on an exhibit for a local museum called â€œThe Titanic Disasterâ€. Theyâ€™ve asked you to analyze the passenger manifests and see if you can find any interesting information for the exhibit.\nThe museum curator is particularly interested in why some people might have been more likely to survive than others."
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_03.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 03",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If youâ€™ve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\n\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\n\n\nimport pandas as pd\nfrom plotnine import *"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-2-initial-exploration",
    "href": "notebooks/Exploration_03.html#part-2-initial-exploration",
    "title": "Data Exploration 03",
    "section": "Part 2: Initial Exploration",
    "text": "Part 2: Initial Exploration\nUsing your visualization library of choice, letâ€™s first look at some features in isolation. Generate visualizations showing:\n\nA comparison of the total number of passengers who survived compared to those that died.\nA comparison of the total number of males compared to females\nA histogram showing the distribution of sibling/spouse counts\nA histogram showing the distribution of parent/child counts\n\n\n# Part 2: # Write the code needed to generate the visualizations specified.\ntitanic = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\")\n\ntitanic.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-3-pairwise-comparisons",
    "href": "notebooks/Exploration_03.html#part-3-pairwise-comparisons",
    "title": "Data Exploration 03",
    "section": "Part 3: Pairwise Comparisons",
    "text": "Part 3: Pairwise Comparisons\nUse your visualization library of choice to look at how the survival distribution varied across different groups.\n\nChoose some features that you think might have had some influence over the likelihood of a titanic passenger surviving.\nFor each of those features, generate a chart for each feature showing the survival distributions when taking that feature into account\n\n\nfrom types import prepare_class\n# Write the code to explore how different features affect the survival distribution\ntitanic_bar = ggplot(titanic, aes(x = 'Survived', fill='factor(Pclass)')) + geom_bar(position='dodge') + theme_minimal()\n\n\ndisplay(titanic_bar)"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-4-feature-engineering",
    "href": "notebooks/Exploration_03.html#part-4-feature-engineering",
    "title": "Data Exploration 03",
    "section": "Part 4: Feature Engineering",
    "text": "Part 4: Feature Engineering\nThe museum curator wonders if the passengerâ€™s rank and title might have anything to do with whether or not they survived. Since this information is embedded in their name, weâ€™ll use â€œfeature engineeringâ€ to create two new columns:\n\nTitle: The passengerâ€™s title\nRank: A boolean (true/false) indicating if a passenger was someone of rank.\n\nFor the first new column, youâ€™ll need to find a way to extract the title portion of their name. Be sure to clean up any whitespace or extra punctuation.\nFor the second new column, youâ€™ll need to first look at a summary of your list of titles and decide what exactly constitutes a title of rank. Will you include military and eccelsiastical titles? Once youâ€™ve made your decision, create the second column.\nYou may want to review prior Data Explorations for tips on creating new columns and checking for lists of values.\n\n# Extract Title from Name\ntitanic[\"Title\"] = titanic[\"Name\"].str.extract(r',\\s*([^\\.]+)\\.')  # This grabs what's after the comma and before the period\ntitanic[\"Title\"] = titanic[\"Title\"].str.strip()  # Remove any whitespace\n\n# Define list of ranked titles (you may choose to include or exclude certain types)\nranked_titles = [\n    \"Countess\", \"Count\", \"Duke\", \"Duchess\", \"Lady\", \"Sir\", \"Baron\", \"Baroness\",\n    \"Lord\", \"Marquis\", \"Marquess\", \"Marchioness\", \"Earl\", \"Viscount\", \"Viscountess\",\n    \"Captain\", \"Major\", \"Colonel\", \"Commander\", \"Lieutenant\",  # Military\n    \"Dr\", \"Rev\", \"Professor\"  # Ecclesiastical / academic\n]\n\n# Create Rank boolean column\ntitanic[\"Rank\"] = titanic[\"Title\"].isin(ranked_titles)\n\n\nRevisit Visualizations\nNow that you have the new columns in place. Revisit the pairwise comparison plots to see if the new columns reveal any interesting relationships.\n\n# Enter the code needed to recheck the pairwise comparison.\n\nggplot(titanic, aes(x='Title', fill='factor(Survived)')) + \\\n    geom_bar(position='fill') + \\\n    labs(x='Title', y='Proportion Survived', fill='Survived') + \\\n    theme_minimal()"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-5-encoding",
    "href": "notebooks/Exploration_03.html#part-5-encoding",
    "title": "Data Exploration 03",
    "section": "Part 5: Encoding",
    "text": "Part 5: Encoding\nThe museum has partnered with a data science group to build some interactive predicitive models using the titanic passenger data.\nMany machine learning algorithms require categorical features to be encoded as numbers.\nThere are two approaches to this, label encoding (sometimes called factorization), and â€œone-hotâ€ encoding.\n\nLabel Encoding\nLabel encoding creates numeric labels for each categorical value. For example, imagine we have a feature in the data called Pet with these values for the first five rows: ['Dog', 'Cat', 'Dog', 'Dog', 'Bird'].\nWe could create a new feature called Pet_Encoded where those values are represented as: [0, 1, 0, 0, 2]. Where 0 = Dog, 1 = Cat, and 2 = Bird.\nIn pandas there are two common ways to label encode a feature:\n\nMethod 1: factorize()\nFirst, we could pandasâ€™ factorize() method. It takes the series you want to encode as an argument and returns a list of two items.\nThe first item is an array of encoded values. The second is the set of original values.\n# The factorize() method returns the new values and the originals in a list.\n# So the [0] at the end indicates we want only the new values.\nmyData['Pet_Encoded'] = pd.factorize(myData['Pet'])[0]\n\n\nMethod 2: Category Data Type\nEvery column in a pandas dataframe is a certain datatype. Usually, pandas infers which datatype to use based on the values of the column. However, we can use the astype() method to convert a feature from one type to another.\nIf we first convert a feature to the category datatype, we can ask pandas to create a new column in the data frame based on the category codes:\n# Convert our column to the category type\nmyData['Pet'] = myData['Pet'].astype('category')\nmyData['Pet_Encoded'] = myData['Pet'].cat.codes\nWhichever method we choose, our machine learning algorithm could use the new Pet_Encoded feature in place of the Pet feature.\n\n# Create a new column in the dataset called \"Sex_Encoded\" containing the\n# label encoded values of the \"Sex\" column\ntitanic[\"Sex_Encoded\"] = pd.factorize(titanic[\"Sex\"])[0]\n\n\n\n\n\nOne-Hot Encoding\nOne problem with label encoding is that it can make a categorical variable appear as if it contains a quantitative relationship between its values.\nIn the example above, is Bird twice as important as Cat? Some algorithms might interpret those values that way.\nOne-Hot encoding avoids this problem by creating a new feature for each category. The value of the new feature is either 0 (is not this value) or 1 (is this value).\nIn pandas, we can use the get_dummies() method to deal with this problem:\nmyEncodedData = pd.get_dummies(myData, columns=['Pet'])\nIn the case of our Pet example, the new features created by get_dummies() would be:\n\n\n\nPet_is_Dog\nPet_is_Cat\nPet_is_Bird\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n0\n0\n\n\n0\n0\n1\n\n\n\nNotice that for our data, if Pet_is_Bird = 0 and Pet_is_Cat = 0, we know that the pet has to be a dog. So the Pet_is_Dog column contains redundant information. When this happens, we say that our data contains a multicollinearity problem.\nTo avoid this, we can tell get_dummies() that we want to get rid of one of the columns using the drop_first parameter:\nmyEncodedData = pd.get_dummies(myData, columns=['Pet'], drop_first=True)\nThe main disadvantage to One-Hot encoding is that if the feature youâ€™re encoding has a lot of different values, it can result in a lot of extra features. This can sometimes lead to poor performance with some types of algorithms.\n\n# Use the pandas get_dummies() method to one-hot encode the Embarked column.\ntitanic = pd.get_dummies(titanic, columns=['Embarked'], drop_first=True)"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-6-conclusions",
    "href": "notebooks/Exploration_03.html#part-6-conclusions",
    "title": "Data Exploration 03",
    "section": "Part 6: Conclusions",
    "text": "Part 6: Conclusions\nBased on your analysis, what interesting relationships did you find? Write three interesting facts the museum can use in their exhibit.\n\ntitle_map = {\n    \"Mr\": \"Mr\",\n    \"Mrs\": \"Mrs\",\n    \"Mme\": \"Mrs\",\n    \"Miss\": \"Miss\",\n    \"Ms\": \"Miss\",\n    \"Mlle\": \"Miss\",\n    \"Master\": \"Master\",\n    \"Dr\": \"Professional\",\n    \"Rev\": \"Professional\",\n    \"Col\": \"Military\",\n    \"Major\": \"Military\",\n    \"Capt\": \"Military\",\n    \"Don\": \"Nobility\",\n    \"Sir\": \"Nobility\",\n    \"Lady\": \"Nobility\",\n    \"Jonkheer\": \"Nobility\",\n    \"the Countess\": \"Nobility\"\n}\n\n# Create the new cleaned title column\ntitanic[\"Title_Clean\"] = titanic[\"Title\"].map(title_map)\n\nggplot(titanic, aes(x='Title_Clean', fill='factor(Survived)')) + \\\n    geom_bar(position='fill') + \\\n    labs(x='Cleaned Title', y='Proportion Survived', fill='Survived') + \\\n    theme_minimal()"
  },
  {
    "objectID": "notebooks/Exploration_03.html#above-and-beyond",
    "href": "notebooks/Exploration_03.html#above-and-beyond",
    "title": "Data Exploration 03",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\n\nThere appears to be a lot of different variations of similar titles. (such as abbreviations for Miss and Mademoiselle).\nScan through the different titles to see which titles can be consolidated, then use what you know about data manipulation to simplify the distribution.\nOnce youâ€™ve finished, check the visualizations again to see if that made any difference.\nThe museum curator has room for a couple of nice visualizations for the exhibit. Create additional visualizations that are suitable for public display."
  },
  {
    "objectID": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "href": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "title": "Data Exploration 01",
    "section": "MPAA Movie Ratings:",
    "text": "MPAA Movie Ratings:\n\nG: All ages admitted.\nPG: Some material may not be suitable for children.\nPG-13: Some material may be inappropriate for children under 13.\nR: Under 17 requires accompanying parent or adult guardian\nNC-17: No One 17 and Under Admitted\n\nMost people would consider G and PG as ratings suitable for children. However, not everyone would agree that a PG-13 movie is necssarily a childrenâ€™s movie. It is up to you to decide how to handle this."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-1-import-pandas",
    "href": "notebooks/Exploration_01.html#part-1-import-pandas",
    "title": "Data Exploration 01",
    "section": "Part 1: Import Pandas",
    "text": "Part 1: Import Pandas\nThe pandas library is a python library used for data analysis and manipulation. It will provide the core functionality for most of what you do in the data exploration and preprocessing stages of most machine learning projects.\nPlease see this Getting Started Guide for information on the conventional way to import Pandas into your project, as well as other helpful tips for common Pandas tasks.\n\n# Part 1: Enter the code below to import Pandas according to the\n# conventional method.\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-2-load-the-data",
    "href": "notebooks/Exploration_01.html#part-2-load-the-data",
    "title": "Data Exploration 01",
    "section": "Part 2: Load the data",
    "text": "Part 2: Load the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 2: Load the dataset into a Pandas dataframe.\nmovies = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\")\n\n\n# Then, explore the data by seeing what the first few rows look like.\nmovies.head()\n\n\n    \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\n81145628\nMovie\nNorm of the North: King Sized Adventure\nRichard Finn, Tim Maltby\nAlan Marriott, Andrew Toth, Brian Dobson, Cole...\nUnited States, India, South Korea, China\nSeptember 9, 2019\n2019\nTV-PG\n90 min\nChildren & Family Movies, Comedies\nBefore planning an awesome wedding for his gra...\n\n\n1\n80117401\nMovie\nJandino: Whatever it Takes\nNaN\nJandino Asporaat\nUnited Kingdom\nSeptember 9, 2016\n2016\nTV-MA\n94 min\nStand-Up Comedy\nJandino Asporaat riffs on the challenges of ra...\n\n\n2\n70234439\nTV Show\nTransformers Prime\nNaN\nPeter Cullen, Sumalee Montano, Frank Welker, J...\nUnited States\nSeptember 8, 2018\n2013\nTV-Y7-FV\n1 Season\nKids' TV\nWith the help of three human allies, the Autob...\n\n\n3\n80058654\nTV Show\nTransformers: Robots in Disguise\nNaN\nWill Friedle, Darren Criss, Constance Zimmer, ...\nUnited States\nSeptember 8, 2018\n2016\nTV-Y7\n1 Season\nKids' TV\nWhen a prison ship crash unleashes hundreds of...\n\n\n4\n80125979\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Next, display a technical summary of the data to determine the data types of each column, and which columns have missing data.\nmovies.info\n\n\n    pandas.core.frame.DataFrame.infodef info(verbose: bool | None=None, buf: WriteBuffer[str] | None=None, max_cols: int | None=None, memory_usage: bool | str | None=None, show_counts: bool | None=None) -&gt; None/usr/local/lib/python3.11/dist-packages/pandas/core/frame.pyPrint a concise summary of a DataFrame.\n\nThis method prints information about a DataFrame including\nthe index dtype and columns, non-null values and memory usage.\n\nParameters\n----------\nverbose : bool, optional\n    Whether to print the full summary. By default, the setting in\n    ``pandas.options.display.max_info_columns`` is followed.\nbuf : writable buffer, defaults to sys.stdout\n    Where to send the output. By default, the output is printed to\n    sys.stdout. Pass a writable buffer if you need to further process\n    the output.\nmax_cols : int, optional\n    When to switch from the verbose to the truncated output. If the\n    DataFrame has more than `max_cols` columns, the truncated output\n    is used. By default, the setting in\n    ``pandas.options.display.max_info_columns`` is used.\nmemory_usage : bool, str, optional\n    Specifies whether total memory usage of the DataFrame\n    elements (including the index) should be displayed. By default,\n    this follows the ``pandas.options.display.memory_usage`` setting.\n\n    True always show memory usage. False never shows memory usage.\n    A value of 'deep' is equivalent to \"True with deep introspection\".\n    Memory usage is shown in human-readable units (base-2\n    representation). Without deep introspection a memory estimation is\n    made based in column dtype and number of rows assuming values\n    consume the same memory amount for corresponding dtypes. With deep\n    memory introspection, a real memory usage calculation is performed\n    at the cost of computational resources. See the\n    :ref:`Frequently Asked Questions &lt;df-memory-usage&gt;` for more\n    details.\nshow_counts : bool, optional\n    Whether to show the non-null counts. By default, this is shown\n    only if the DataFrame is smaller than\n    ``pandas.options.display.max_info_rows`` and\n    ``pandas.options.display.max_info_columns``. A value of True always\n    shows the counts, and False never shows the counts.\n\nReturns\n-------\nNone\n    This method prints a summary of a DataFrame and returns None.\n\nSee Also\n--------\nDataFrame.describe: Generate descriptive statistics of DataFrame\n    columns.\nDataFrame.memory_usage: Memory usage of DataFrame columns.\n\nExamples\n--------\n&gt;&gt;&gt; int_values = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n&gt;&gt;&gt; float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n&gt;&gt;&gt; df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n...                   \"float_col\": float_values})\n&gt;&gt;&gt; df\n    int_col text_col  float_col\n0        1    alpha       0.00\n1        2     beta       0.25\n2        3    gamma       0.50\n3        4    delta       0.75\n4        5  epsilon       1.00\n\nPrints information of all columns:\n\n&gt;&gt;&gt; df.info(verbose=True)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   int_col    5 non-null      int64\n 1   text_col   5 non-null      object\n 2   float_col  5 non-null      float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes\n\nPrints a summary of columns count and its dtypes but not per column\ninformation:\n\n&gt;&gt;&gt; df.info(verbose=False)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nColumns: 3 entries, int_col to float_col\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes\n\nPipe output of DataFrame.info to buffer instead of sys.stdout, get\nbuffer content and writes to a text file:\n\n&gt;&gt;&gt; import io\n&gt;&gt;&gt; buffer = io.StringIO()\n&gt;&gt;&gt; df.info(buf=buffer)\n&gt;&gt;&gt; s = buffer.getvalue()\n&gt;&gt;&gt; with open(\"df_info.txt\", \"w\",\n...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n...     f.write(s)\n260\n\nThe `memory_usage` parameter allows deep introspection mode, specially\nuseful for big DataFrames and fine-tune memory optimization:\n\n&gt;&gt;&gt; random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n... })\n&gt;&gt;&gt; df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 22.9+ MB\n\n&gt;&gt;&gt; df.info(memory_usage='deep')\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 165.9 MB"
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "href": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "title": "Data Exploration 01",
    "section": "Part 3: Filter the Data",
    "text": "Part 3: Filter the Data\nSince weâ€™re just interested in movies, weâ€™ll need to filter out anything that isnâ€™t a movie for our analysis. The type feature contains this information.\nOnce we have the subset, we should see how many rows it contains. There are a variety of ways to get the length of a data frame.\n\n# Use pandas's filtering abilitites to select the subset of data\n# that represents movies, then calculate how many rows are in the filtered data.\nonly_movies = [movies[type] == \"Movie\"]\n\nonly_movies.index\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py in get_loc(self, key)\n   3804         try:\n-&gt; 3805             return self._engine.get_loc(casted_key)\n   3806         except KeyError as err:\n\nindex.pyx in pandas._libs.index.IndexEngine.get_loc()\n\nindex.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: &lt;class 'pandas.core.frame.DataFrame'&gt;\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-9-d1d294c15664&gt; in &lt;cell line: 0&gt;()\n      1 # Use pandas's filtering abilitites to select the subset of data\n      2 # that represents movies, then calculate how many rows are in the filtered data.\n----&gt; 3 only_movies = [movies[type] == \"movies\"]\n      4 \n      5 only_movies.index\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py in __getitem__(self, key)\n   4100             if self.columns.nlevels &gt; 1:\n   4101                 return self._getitem_multilevel(key)\n-&gt; 4102             indexer = self.columns.get_loc(key)\n   4103             if is_integer(indexer):\n   4104                 indexer = [indexer]\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py in get_loc(self, key)\n   3810             ):\n   3811                 raise InvalidIndexError(key)\n-&gt; 3812             raise KeyError(key) from err\n   3813         except TypeError:\n   3814             # If we have a listlike key, _check_indexing_error will raise\n\nKeyError: &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\nMPAA Ratings\nNow that we have only movies, letâ€™s get a quick count of the values being used in the rating feature.\n\n# Determine the number of records for each value of the \"rating\" feature.\n# Remember to count the values in your subset only, not in the original dataframe.\n\n\n\nMore Filtering\nThere are apparently some â€œmade for TVâ€ movies in the list that donâ€™t fit the MPAA rating scheme.\nLetâ€™s filter some more to just see movies rated with the standard MPAA ratings of G, PG, PG-13, R, and NC-17.\n\n# Filter the list of movies to select a new subset containing only movies with\n# a standard MPAA rating. Calculate how many rows are in this new set, and\n# then see which ratings appear most often."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-4-visualization",
    "href": "notebooks/Exploration_01.html#part-4-visualization",
    "title": "Data Exploration 01",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nNow that we have explored and preprocessed our data, letâ€™s create a visualization to summarize our findings.\n\nExploration vs Presentation\nBroadly speaking, there are two types of visualizations: * Barebones visualizations you might use to get a quick, visual understanding of the data while youâ€™re trying to decide how it all fits together. * Presentation-quality visualizations that you would include in a report or presentation for management or other stakeholders.\n\n\nVisualization Tools\nThere are many different visualization tools availble. In the sections below, weâ€™ll explore the three most common. Each of these libraries has strengths and weaknesses.\nIt is probably a good idea for you to become familiar with each one, and then become proficient at whichever one you like the best.\n\n\nAltair\nThe Altair visualization library provides a large variety of very easy to use statistical charting tools.\nAltair uses a declarative language to build up charts piece by piece.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nalt.Chart(employees).mark_boxplot().encode(\n    x='job',\n    y='salary'\n)\n\n# Make a box plot style categorical plot, and customize the results\nalt.Chart(employees).mark_boxplot().encode(\n    alt.X('job', title='Job title'),\n    alt.Y('salary', title='Annual salary in thousands of $USD')\n).properties(\n  title='Salaries by Job Title'\n)\nLike with Pandas, there is a conventional way to import Altair into your projects.\n\n# Import the Altair library the conventional way.\n\nLetâ€™s create a barchart showing the count of each movie rating by using Altairâ€™s aggregation capabilities.\nIn this example, we see the x axis being set to a feature called a, and the y axis set to the average() of a feature called b.\nIn our case, we want the x axis to be set to rating and the y axis to be the count() of rating.\n\n# Use Altair to create a bar chart comparing the count of each movie rating\n\n\n\nSeaborn\nWhile Altair uses a â€œdeclarativeâ€ syntax for building charts piece by piece, the Seaborn library provides a large variety of pre-made charts for common statistical needs.\nThese charts are divided into different categories. Each category has a high-level interface you can use for simplicity, and then a specific function for each chart that you can use if you need more control over how the chart looks.\nSeaborn uses matplotlib for its drawing, and the chart-specific functions each return a matplitlib axes object if you need additional customization.\nFor example, there are several different types of categorical plots in seaborn: bar plots, box plots, point plots, count plots, swarm plots, etcâ€¦\nEach of these plots can be accessed using the catplot function.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nsns.catplot(data=employees, x='job', y='salary', kind='box')\n\n# Make a swarm plot style categorical plot\nsns.catplot(data=employees, x='job', y='salary', kind='swarm')\nAlternatively, you can use the plot specific functions to give yourself more control over the output by using matplotlib functions:\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = sns.boxplot(data=employees, x='job', y='salary')\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\nLike with Pandas, there is a conventional way to import Seaborn into your projects.\nOptionally, you may wish to set some default chart aesthetics by setting the chart style.\n\n# Import the seaborn library the conventional way. Then optionally configure\n# the default chart style.\n\nSince the rating column uses categorical data, we need to use Seabornâ€™s categorical visualizations.\nIn particular, we want a â€œcount plotâ€ that will display a count of movie ratings.\n\n# Use seaborn to create a count plot comparing the count of each movie rating\n\n\n\nPandas built-in plotting\nIn addition to libraries like Altair and Seaborn, Pandas has some built in charting functionality.\nWhile not as sophisticated as some of the other options, it is often good enough for quick visualizations.\nJust like with seabornâ€™s plotting functions, the pandas plotting functions return matplotlib axes objects, which can be further customized.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nemployees[ ['job','salary'] ].plot.box()\n\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = employees[ ['job','salary'] ].plot().box()\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\n\n# Use pandas' built in plotting functions to create a count plot comparing the count of each movie rating\n# This will be a little trickier than the other libraries, but one hint is that the pandas value_counts() function\n# actually returns a dataframe."
  },
  {
    "objectID": "notebooks/Exploration_01.html#above-and-beyond",
    "href": "notebooks/Exploration_01.html#above-and-beyond",
    "title": "Data Exploration 01",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nAfter reviewing your findings, the watchdog group would like some additional questions answered:\n\nHow are things affected if you include the â€œmade for TV moviesâ€ that have been assigned TV ratings in your analysis, but still exclude unrated movies?\nThey would also like to see a separate report that includes only TV shows.\nFor an upcoming community meeting, the group would like to present a simple chart showing â€œFor Kidsâ€ and â€œFor Adultsâ€ categories. The easiest way to accomplish this would be to create a new column in your data frame that maps each rating to the appropriate â€œFor Kidsâ€ or â€œFor Adultsâ€ label, then create a new visualization based on that column."
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  }
]